# Original Work: https://github.com/Azure-Samples/terraform-github-actions/blob/main/.github/workflows/tf-plan-apply.yml

name: '[Reusable] Terraform Destroy'

on:
  workflow_call:
    inputs:
      ACT_MODE:
        description: |
          For running Github Action Workflows locally with Nektos act
        required: true
        type: boolean
      DATABRICKS_ADMINISTRATOR:
        description: |
          Databricks Accounts Administrator user account (not for machine access)
        required: true
        type: string
      DATABRICKS_PROFILE:
        description: |
          Databricks CLI configuration profile name for Databricks Accounts configuration/credentials
        required: false
        type: string
        default: AWS_ACCOUNTS
      mask:
        description: |
          Flag to mask environment variables
        required: false
        type: boolean
        default: true
      state_remove:
        description: |
          Optional State to remove prior to destroy step
        required: false
        type: string
        default: ''
      working_directory:
        description: |
          Working directory containing Terraform test scripts.
        required: false
        type: string
        default: "./test"
      token_lifetime:
        description: |
          AWS STS Token Lifetime in seconds
        required: false
        type: number
        default: 1200
      token_retries:
        description: |
          AWS STS Token request retry attempts
        required: false
        type: number
        default: 3
    
    secrets:
      env_variables:
        description: |
          Passing ENV Variables from parent level workflow to reusable workflow
          https://github.com/orgs/community/discussions/26671#discussioncomment-6776498

          Possible ENV Variables include
          secrets:
            env_variables:
              TF_VAR_WIF_ID="$\{{ github.run_id}}-$\{{github.run_attempt}}"
              TF_VAR_GITHUB_REPOSITORY_OWNER="$\{{ github.repository_owner }}"
              TF_VAR_GITHUB_REPOSITORY="$\{{ github.repository }}"
              TF_VAR_GITHUB_REF="$\{{ github.ref }}"
              TF_VAR_GITHUB_ENV=production

        required: false
      AWS_CLIENT_ID:
        description: |
          AWS Service Account Client ID for authentication
        required: true
      AWS_CLIENT_SECRET:
        description: |
          AWS Service Account Client Secret for Authentication (when running locally)
        required: true
      AWS_REGION:
        description: |
          AWS Account Region
        required: false
      AWS_ROLE_TO_ASSUME:
        description: |
          AWS role to assume when asking for Security Token Service (STS) to provide a set of temporary credentials
        required: false
      TF_API_TOKEN:
        description: |
          Terraform.io Access Token
        required: true
      AWS_DATABRICKS_ACCOUNT_ID:
        description: |
          AWS Databricks Accounts Account ID
        required: true
      AWS_DATABRICKS_ACCOUNT_CLIENT_ID:
        description: |
          AWS Databricks Accounts Client ID with Admin Permissions
        required: true
      AWS_DATABRICKS_ACCOUNT_CLIENT_SECRET:
        description: |
          AWS Databricks Accounts Client Secret with Admin Permissions
        required: true

#Special permissions required for OIDC authentication
permissions:
  id-token: write
  contents: read

env:
  TF_VAR_DATABRICKS_ADMINISTRATOR: "${{ inputs.DATABRICKS_ADMINISTRATOR }}"
  TF_VAR_DATABRICKS_CLI_PROFILE: "${{ inputs.DATABRICKS_PROFILE }}"
  TF_VAR_DATABRICKS_ACCOUNT_ID: "${{ secrets.AZURE_DATABRICKS_ACCOUNT_ID }}"

jobs:
  reusable-integration-destroy:
    name: 'Terraform Destroy'
    environment: production
    runs-on: ubuntu-latest

    defaults:
      run:
        working-directory: "${{ inputs.working_directory }}"

    steps:
    # Checkout the repository to the GitHub Actions runner
    - name: Checkout
      uses: actions/checkout@v4
    
    - name: Decode Environment Variables
      env:
        env_vars: ${{ secrets.env_variables }}
      run: |
        for i in $env_vars; do
          i=$(echo $i | sed 's/=.*//g')=$(echo ${i#*=} | base64 -di | base64 -di)
          if ${{ inputs.mask }}
          then
            echo ::add-mask::${i#*=}
          fi
          printf '%s\n' "$i" >> $GITHUB_ENV
        done
    
    - name: Configure AWS Credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ !!inputs.ACT_MODE && secrets.AWS_CLIENT_ID || null }}
        aws-secret-access-key: ${{ !!inputs.ACT_MODE && secrets.AWS_CLIENT_SECRET || null }}
        aws-region: ${{ secrets.AWS_REGION }}
        role-to-assume: ${{ secrets.AWS_ROLE_TO_ASSUME }}
        role-duration-seconds: ${{ inputs.token_lifetime }}
        role-skip-session-tagging: true
        mask-aws-account-id: true
        retry-max-attempts: ${{ inputs.token_retries }}
    
    # Install the latest version of Terraform CLI and configure the Terraform CLI configuration file with a Terraform Cloud user API token
    - name: Setup Terraform
      uses: hashicorp/setup-terraform@v3
      with:
        cli_config_credentials_token: ${{ secrets.TF_API_TOKEN }}
    
    # Databricks offers Workload Identity Federation from Databricks Accounts
    # to Databricks Workspaces via SCIM connectors to cloud providers. However,
    # Github is not a recognized Identity Federation/Open ID Connect from Databricks Accounts
    # authorization at this time. Therefore, providing client secrets via GA
    # Secrets is necessary until advances are made with Databricks Accounts WIF/OIDC.
    - name: Databricks Accounts Configuration
      run: |
        echo "[${{ inputs.DATABRICKS_PROFILE }}]
        host          = https://accounts.cloud.databricks.com
        account_id    = ${{ secrets.AWS_DATABRICKS_ACCOUNT_ID }}
        client_id     = ${{ secrets.AWS_DATABRICKS_ACCOUNT_CLIENT_ID }}
        client_secret = ${{ secrets.AWS_DATABRICKS_ACCOUNT_CLIENT_SECRET }}
        jobs-api-version = 2.1" > ~/.databrickscfg
    
    # Initialize a new or existing Terraform working directory by creating initial files, loading any remote state, downloading modules, etc.
    - name: Terraform Init
      run: terraform init
    
    # Terraform archive_file fails on destroy when source files can't be found
    # These data.archive_file.this resources need to be removed from state
    # prior to the destroy activity
    - name: Archives
      id: archives
      run: |
        echo "ARCHIVES=$(
          terraform state list | 
          grep 'data.archive_file.this\|null_resource.this' | 
          sed 's/\"/\\\"/g' | 
          paste -s -d, -)" >> $GITHUB_OUTPUT
    
    - name: Terraform State Remove
      if: ${{ inputs.state_remove  != '' || steps.archives.outputs.ARCHIVES != '' }}
      run: |
        targets=($(echo "${{ steps.archives.outputs.ARCHIVES }},${{ inputs.state_remove }}" | tr ',' '\n'))
        for t in ${targets[@]}
        do
          if [ $t != '' ]
          then
            terraform state rm $t
          fi
        done
    
    # Terraform Destroy
    - name: Terraform Destroy
      uses: nick-fields/retry@v3
      with:
        timeout_minutes: 5
        max_attempts: 3
        retry_on: error
        command: 'terraform -chdir=test destroy --auto-approve'